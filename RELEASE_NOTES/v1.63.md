## Summary
- **Fixed**: Addressed [Issue #21](https://github.com/ussoewwin/ComfyUI-QwenImageLoraLoader/issues/21) â€“ CPU offload setting was hardcoded to `"auto"`, causing unnecessary slowdowns
- **Problem**: Users could not control CPU offload behavior, and the setting was always `"auto"`, which automatically enabled CPU offload when VRAM was low, causing slowdowns even when VRAM was sufficient
- **Solution**: Added `cpu_offload` parameter to `INPUT_TYPES` allowing users to select from `["auto", "enable", "disable"]` with default `"disable"` for performance

## Problem
- CPU offload setting was hardcoded to `"auto"` in the initialization code
- When `"auto"` mode detected low VRAM (< 4.0GB), CPU offload was automatically enabled
- This caused slowdowns even when users had sufficient VRAM
- Users had no way to explicitly disable CPU offload for better performance
- Reported by: @G7b9 (GitHub Issue #21)

## Technical Solution
- Added `cpu_offload` parameter to `INPUT_TYPES()` for both `NunchakuQwenImageLoraLoader` and `NunchakuQwenImageLoraStack`
- Options: `["auto", "enable", "disable"]`
- Default: `"disable"` (performance-first)
- Updated method signatures to accept `cpu_offload` parameter
- Pass user-selected value to `ComfyQwenImageWrapper` instead of hardcoded `"auto"`
- Added `cpu_offload` to `IS_CHANGED()` method for proper cache management
- Added comprehensive logging to track CPU offload settings

## Code Changes

### Files Modified
1. `nodes/lora/qwenimage.py` - Main LoRA loader node implementation
2. `wrappers/qwenimage.py` - Added initialization logging

### Key Changes

#### 1. Added `cpu_offload` Parameter to `INPUT_TYPES()`

**NunchakuQwenImageLoraLoader:**
```python
"cpu_offload": (
    ["auto", "enable", "disable"],
    {
        "default": "disable",
        "tooltip": "CPU offload setting. 'auto' enables offload when VRAM is low, 'enable' forces offload, 'disable' disables offload.",
    },
),
```

**NunchakuQwenImageLoraStack:**
- Same implementation as `NunchakuQwenImageLoraLoader`

#### 2. Updated Method Signatures

**Before:**
```python
def load_lora(self, model, lora_name: str, lora_strength: float):
```

**After:**
```python
def load_lora(self, model, lora_name: str, lora_strength: float, cpu_offload: str):
```

#### 3. Updated `ComfyQwenImageWrapper` Creation

**Before:**
```python
wrapped_model = ComfyQwenImageWrapper(
    model_wrapper,
    getattr(model_wrapper, 'config', {}),
    None,
    {},
    "auto",  # â† Hardcoded
    4.0,
)
```

**After:**
```python
wrapped_model = ComfyQwenImageWrapper(
    model_wrapper,
    getattr(model_wrapper, 'config', {}),
    None,
    {},
    cpu_offload,  # â† User-selected value
    4.0,
)
```

#### 4. Added Cache Management

**Updated `IS_CHANGED()` method:**
```python
@classmethod
def IS_CHANGED(s, model, lora_name, lora_strength, cpu_offload, *args, **kwargs):
    import hashlib
    m = hashlib.sha256()
    m.update(lora_name.encode())
    m.update(str(lora_strength).encode())
    m.update(str(model).encode())
    m.update(cpu_offload.encode())  # â† Added
    return m.digest().hex()
```

#### 5. Added Logging

**Wrapper Initialization (`wrappers/qwenimage.py`):**
```python
logger.info(f"ðŸ”§ CPU offload setting: '{cpu_offload_setting}' (VRAM margin: {vram_margin_gb}GB)")
```

**Wrapper Creation (`nodes/lora/qwenimage.py`):**
```python
logger.info(f"ðŸ“¦ Creating ComfyQwenImageWrapper with cpu_offload='{cpu_offload}'")
```

**Setting Update:**
```python
if model_wrapper.cpu_offload_setting != cpu_offload:
    logger.info(f"ðŸ”„ Updating CPU offload setting from '{model_wrapper.cpu_offload_setting}' to '{cpu_offload}'")
    model_wrapper.cpu_offload_setting = cpu_offload
```

## Behavior Changes

### CPU Offload Options

1. **`"auto"`** - Automatically enables CPU offload when VRAM is below 4.0GB margin
   - Same behavior as before when `"auto"` was hardcoded
   - Recommended for users with limited VRAM

2. **`"enable"`** - Always enables CPU offload
   - Forces CPU offload regardless of VRAM availability
   - Useful for very low VRAM systems

3. **`"disable"`** - Always disables CPU offload (default)
   - Keeps all processing on GPU for maximum performance
   - Recommended for users with sufficient VRAM

### Default Behavior

- **Previous**: Always `"auto"` (could cause slowdowns)
- **Current**: `"disable"` (performance-first, users can change if needed)

## Benefits

- âœ… **User Control**: Users can now explicitly control CPU offload behavior
- âœ… **Performance**: Default `"disable"` setting provides better performance for users with sufficient VRAM
- âœ… **Flexibility**: Users with low VRAM can still use `"auto"` or `"enable"` modes
- âœ… **Visibility**: Comprehensive logging helps users verify CPU offload settings
- âœ… **Cache Management**: Proper cache invalidation when CPU offload setting changes

## Backward Compatibility

- âš ï¸ **Breaking Change**: Existing workflows need to be updated
  - The `cpu_offload` parameter is now required
  - Default value is `"disable"` (different from previous `"auto"` behavior)
  - Users should review their workflows and adjust the `cpu_offload` setting if needed

## Migration Guide

### For Users with Sufficient VRAM (Recommended)
- No action required - default `"disable"` setting provides best performance
- If you experience VRAM issues, change to `"auto"` or `"enable"`

### For Users with Limited VRAM
- Change `cpu_offload` setting to `"auto"` or `"enable"` in the node UI
- This restores the previous automatic behavior

### For Existing Workflows
- Open your workflow in ComfyUI
- The `cpu_offload` parameter will appear in LoRA loader nodes
- Select your preferred setting (`"disable"` for performance, `"auto"` for automatic, `"enable"` for forced offload)
- Save your workflow

## Testing

### Verified Scenarios
- âœ… New wrapper creation with different `cpu_offload` settings
- âœ… Existing wrapper setting update
- âœ… Cache invalidation when `cpu_offload` setting changes
- âœ… Logging output for all scenarios
- âœ… Both `NunchakuQwenImageLoraLoader` and `NunchakuQwenImageLoraStack` nodes

### Log Output Examples

**New Wrapper Creation:**
```
ðŸ”§ Wrapping NunchakuQwenImageTransformer2DModel with ComfyQwenImageWrapper
ðŸ“¦ Creating ComfyQwenImageWrapper with cpu_offload='disable'
ðŸ”§ CPU offload setting: 'disable' (VRAM margin: 4.0GB)
```

**Existing Wrapper:**
```
âœ… Model is already wrapped (detected via attributes)
ðŸ“¦ Current CPU offload setting: 'auto'
ðŸ”„ Updating CPU offload setting from 'auto' to 'disable'
```

## Acknowledgments

- **Reported by**: @G7b9 (GitHub Issue #21)
- **Special Thanks**: This improvement was made possible by @G7b9's detailed problem report and feature request

## Links

- **Issue #21**: https://github.com/ussoewwin/ComfyUI-QwenImageLoraLoader/issues/21
- **Tags page**: https://github.com/ussoewwin/ComfyUI-QwenImageLoraLoader/tags
- **v1.63 release**: https://github.com/ussoewwin/ComfyUI-QwenImageLoraLoader/releases/tag/v1.63

